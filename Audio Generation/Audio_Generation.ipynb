{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1ESrNRLE37V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0a43b1-3b9e-43b6-8be7-0fd759957777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lpobqtqBK4w0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/Drive/MyDrive/kaggle'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Dataset"
      ],
      "metadata": {
        "id": "kw52XmNIL0ie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLqFSfUFK-iJ",
        "outputId": "db90c7ea-feb9-4350-c077-59642e0f5e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading basic-arabic-vocal-emotions-dataset.zip to /content\n",
            " 87% 121M/138M [00:01<00:00, 102MB/s]\n",
            "100% 138M/138M [00:01<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d a13x10/basic-arabic-vocal-emotions-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HepLeWXwLEp_"
      },
      "outputs": [],
      "source": [
        "# extract the dataset from the zipfile\n",
        "import zipfile\n",
        "\n",
        "file_path = '/content/basic-arabic-vocal-emotions-dataset.zip'\n",
        "\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/basic-arabic-vocal-emotions-dataset/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Import the necessary Libraries***"
      ],
      "metadata": {
        "id": "WAKv-iglL84O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "akdSBPioHJyt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import librosa\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "L9wsoZaHKsO_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Generator Class***"
      ],
      "metadata": {
        "id": "eONa5aqkMJ4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nwag_2QkI8sM"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Sequential model for the generator\n",
        "        self.gen = nn.Sequential(\n",
        "            # First convolutional block\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),\n",
        "\n",
        "            # Second convolutional block\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),\n",
        "\n",
        "            # Third convolutional block\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),\n",
        "\n",
        "            # Fourth convolutional block\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),\n",
        "\n",
        "            # Final convolutional layer with transpose operation\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=2, stride=2, padding=1\n",
        "            ),\n",
        "\n",
        "            # Tanh activation function to output audio in range [-1, 1]\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        \"\"\"Helper function to define a convolutional block with transpose convolution, batch normalization, and ReLU activation.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            # Transpose convolutional layer\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            # Batch normalization to stabilize training\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            # ReLU activation function to introduce non-linearity\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the generator.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.gen(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Discriminator Class***"
      ],
      "metadata": {
        "id": "sdsaoBvYNKX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "esvBys6gJBoh"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Sequential model for the discriminator\n",
        "        self.disc = nn.Sequential(\n",
        "            # First convolutional layer\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=2, stride=2, padding=1),\n",
        "            # LeakyReLU activation function to introduce non-linearity\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # Second convolutional block\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            # Third convolutional block\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            # Fourth convolutional block\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # Final convolutional layer\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=2, stride=2, padding=0),\n",
        "            # Sigmoid activation function to output probability\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        \"\"\"\n",
        "        Helper function to define a convolutional block with convolution, batch normalization, and LeakyReLU activation.\n",
        "\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            # Convolutional layer\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            # Batch normalization to stabilize training\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            # LeakyReLU activation function to introduce non-linearity\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the discriminator.\n",
        "\n",
        "        \"\"\"\n",
        "        return self.disc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "medBFh4YJEPv"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Initialise Hyperparameters***"
      ],
      "metadata": {
        "id": "16grXTWdNScD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cGRBUfu9JGoA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 64\n",
        "channels_noise = 64\n",
        "channels_img=  1\n",
        "features_g = 64\n",
        "features_d = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X88-HbeCJUaM"
      },
      "outputs": [],
      "source": [
        "# Instantiate the generator and move it to the device (GPU or CPU)\n",
        "gen = Generator(channels_noise, channels_img, features_g).to(device)\n",
        "\n",
        "# Instantiate the discriminator and move it to the device (GPU or CPU)\n",
        "disc = Discriminator(channels_img, features_d).to(device)\n",
        "\n",
        "# Initialize the weights of the generator and discriminator\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ekQX4ZPwMz9j"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer for the generator, specifying its parameters, learning rate, and beta values\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "# Define the optimizer for the discriminator, specifying its parameters, learning rate, and beta values\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "# Define the binary cross-entropy loss function\n",
        "criterion = nn.BCELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "F8xu8monM3uL"
      },
      "outputs": [],
      "source": [
        "# Function to extract MFCC features from the audio files\n",
        "def preprocess_wav(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    return mfccs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vPfV0aENNdvk"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the dataset\n",
        "filename = '/content/basic-arabic-vocal-emotions-dataset/remake'\n",
        "\n",
        "# Initialize an empty list to store the extracted features\n",
        "feats = []\n",
        "\n",
        "# Iterate over the directories in the dataset directory\n",
        "for i in os.listdir(filename):\n",
        "    # Check if the directory name has a length of 1\n",
        "    if len(i) == 1:\n",
        "        # Construct the full path of the subdirectory\n",
        "        filepath = os.path.join(filename, i)\n",
        "        # Iterate over the files in the subdirectory\n",
        "        for j in os.listdir(filepath):\n",
        "            # Construct the full path of the audio file\n",
        "            aud_path = os.path.join(filepath, j)\n",
        "            # Extract MFCC features from the audio file\n",
        "            mfcc = preprocess_wav(aud_path)\n",
        "            # Append a tuple containing the filename and its corresponding MFCC features to the feats list\n",
        "            feats.append((j, mfcc))\n",
        "\n",
        "# Shuffle the list of tuples to randomize the order of the data\n",
        "random.shuffle(feats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvMkAKjWPMZX",
        "outputId": "aaa719a9-7a16-4d05-bb43-ea5c6f630224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1935"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N9wbOpIMRRXU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Define a function to pad or truncate MFCC features\n",
        "def pad_truncate(features, max_len):\n",
        "    if len(features[0]) < max_len:\n",
        "        padded = [torch.cat([torch.tensor(feature), torch.zeros(max_len - len(feature))], dim=0) for feature in features]\n",
        "    else:\n",
        "        padded = [torch.tensor(feature)[:max_len] for feature in features]\n",
        "    return torch.stack(padded)\n",
        "\n",
        "# Assuming feats is a list of tuples (filename, mfcc)\n",
        "max_len = max(len(mfcc) for _, mfcc in feats)  # Find the maximum length of MFCC features\n",
        "\n",
        "# Pad or truncate MFCC features\n",
        "padded_feats = [(filename, pad_truncate(mfcc, max_len)) for filename, mfcc in feats]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ryzx3-0qPxlr"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(padded_feats, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to save generated audio\n",
        "def save_generated_audio(generated_audio, epoch, batch_idx):\n",
        "    for i, audio in enumerate(generated_audio):\n",
        "        output_file_path = f\"generated_audio_epoch{epoch}_batch{batch_idx}_example{i}.wav\"\n",
        "        sf.write(output_file_path, audio.squeeze().detach().cpu().numpy(), sr)"
      ],
      "metadata": {
        "id": "h6nxEM63IULg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqALs10yQlWM",
        "outputId": "52ccb601-08b9-4b5e-8227-c9de936b05aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5] Batch 0/31                   Loss D: 0.6923, loss G: 0.7103\n",
            "Epoch [1/5] Batch 0/31                   Loss D: 0.4358, loss G: 0.9853\n",
            "Epoch [2/5] Batch 0/31                   Loss D: 0.2806, loss G: 1.3051\n",
            "Epoch [3/5] Batch 0/31                   Loss D: 0.1699, loss G: 1.7126\n",
            "Epoch [4/5] Batch 0/31                   Loss D: 0.1039, loss G: 2.1440\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "sr = 16000\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (filename, audio_features) in enumerate(train_loader):\n",
        "        # Move audio features to device\n",
        "        audio_features = audio_features.unsqueeze(1).to(device)\n",
        "\n",
        "        ### Train Discriminator ###\n",
        "        opt_disc.zero_grad()\n",
        "\n",
        "        # Generate fake audio\n",
        "        noise = torch.randn(len(audio_features), channels_noise, 1, 1).to(device)\n",
        "        fake_audio = gen(noise)\n",
        "\n",
        "        # Train discriminator on real audio\n",
        "        disc_real_output = disc(audio_features).reshape(-1)\n",
        "        disc_real_loss = criterion(disc_real_output, torch.ones_like(disc_real_output))\n",
        "\n",
        "        # Train discriminator on fake audio\n",
        "        disc_fake_output = disc(fake_audio.detach()).reshape(-1)\n",
        "        disc_fake_loss = criterion(disc_fake_output, torch.zeros_like(disc_fake_output))\n",
        "\n",
        "        # Total discriminator loss\n",
        "        disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
        "\n",
        "        # Backpropagation\n",
        "        disc_loss.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator ###\n",
        "        opt_gen.zero_grad()\n",
        "\n",
        "        # Generate fake audio\n",
        "        fake_audio = gen(noise)\n",
        "\n",
        "        # Get discriminator's prediction on fake audio\n",
        "        disc_fake_output = disc(fake_audio).reshape(-1)\n",
        "\n",
        "        # Generator loss\n",
        "        gen_loss = criterion(disc_fake_output, torch.ones_like(disc_fake_output))\n",
        "\n",
        "        # Backpropagation\n",
        "        gen_loss.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
        "                  Loss D: {disc_loss:.4f}, loss G: {gen_loss:.4f}\"\n",
        "            )\n",
        "\n",
        "        # Save generated audio every few batches\n",
        "        if batch_idx % 500 == 0:\n",
        "            with torch.no_grad():\n",
        "                gen.eval()\n",
        "                generated_audio = gen(torch.randn(5, channels_noise, 1, 1).to(device))\n",
        "                save_generated_audio(generated_audio, epoch, batch_idx)\n",
        "                gen.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}