{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset and its structure**\n",
    "\n",
    "1. Commonly known datasets that can be used for trianing purpose:\n",
    "    1. Flickr8K\n",
    "    2. Flick30K\n",
    "    3. Fick100K\n",
    "    4. MSCOCO\n",
    "2. Each dataset may have there own structure of dataset. For Flickr_8K dataset, all the images of training, validation and test set are in one folder. It contains 3 different files i.e Flickr_8k.trainImages.txt, Flickr_8k.testImages.txt , Flickr_8k.devImages.txt  corresponding to each type of dataset i.e train, test and validation set, each file having file_name of images conatined in each dataset. \n",
    "3. For example, in Flick8k, Flickr_8k.trainImages.txt file contains file_ids of images in training set. Name of image file is its image id.\n",
    "4. All the images are in same folder. So to parse images of training dataset, first read trianImages.txt file, read line by line image id and load corresponding image from image dataset folder.\n",
    "5. Each image is given 5 different captions by 5 different humans. This is because an image can be described in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ranINlZl1IpO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lFTmpBiMHL7S"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, concatenate, Dropout\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Add\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U7f--mwM99Rv"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OA0RpF-30L4i"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3L_qhpoLrJeq"
   },
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GQAgsJ4nCF6",
    "outputId": "d278398e-6519-474e-fec2-9979d1e84108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2NjuzWpzwzi",
    "outputId": "2b1d1677-3b0f-4d99-e34e-7f29221a7264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)# summarize\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D3oizWRHmQP",
    "outputId": "49f3fc03-ba31-4973-9ded-56550daabe2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=load_img('/content/gdrive/MyDrive/archive/Images/3767841911_6678052eb6.jpg',target_size=(224, 224))\n",
    "img_to_array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm_YC8g30OIT"
   },
   "outputs": [],
   "source": [
    "def img_preprocess(filename):\n",
    "  feats={}\n",
    "  j=0\n",
    "  for img in os.listdir(filename):\n",
    "    image=load_img(f'{filename}/{img}',target_size=(224, 224))\n",
    "    img_arr=img_to_array(image)\n",
    "    img_arr = img_arr.reshape((1, img_arr.shape[0], img_arr.shape[1], img_arr.shape[2]))\n",
    "    feat=model.predict(img_arr,verbose=0)#no output shown as verbose=0\n",
    "    img_id=img.split('.')[0]\n",
    "    feats[img_id]=feat\n",
    "    print(j)\n",
    "    j+=1\n",
    "  return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-N0HwviIa5G"
   },
   "outputs": [],
   "source": [
    " features=img_preprocess('/content/gdrive/MyDrive/archive/Images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TakztxzL9fRa"
   },
   "outputs": [],
   "source": [
    " pkl_file_path = '/content/drive/MyDrive/archive/your_data.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aHfK6i-XvPeo"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    " pkl_file_path = '/content/drive/MyDrive/archive/your_data.pkl'\n",
    " with open(pkl_file_path, 'wb') as pkl_file:\n",
    "    pickle.dump(features, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2DeQUCa4v9Hn"
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "all_features = load(open('your_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "952lu54h6z2r",
    "outputId": "482d157f-62d6-4c8c-e60a-4bb9ab90b051"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8101"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(all_features.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Loading and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "rZ_yDMHKwIm9",
    "outputId": "350f7d4b-63d2-43b0-aaee-ad64d80be3f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image,caption\\n1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg,A girl going into a wooden building .\\n1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\\n1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\\n1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\\n1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting\\n1001773457_577c3a7d70.jpg,A black dog and a tri-colored dog playing with each other on the road .\\n1001773457_577c3a7d70.jpg,A black dog and a white dog with brown spots are staring at each other in the street .\\n1001773457_577c3a7d70.jpg,Two dogs of different breeds looking at each other on the road .\\n1001773457_577c3a7d70.jpg,Two dogs on pavement moving toward each other .\\n1002674143_1b742ab4b8.jpg,A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\\n1002674143_1b742ab4b8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path='/content/drive/MyDrive/archive/captions.txt'\n",
    "with open('archive/captions.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "(content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vp3ZzE3O-GNf"
   },
   "outputs": [],
   "source": [
    "s = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9nx65a5K9kR-"
   },
   "outputs": [],
   "source": [
    "def clean(caption):\n",
    "    cleaned = []\n",
    "    x=\"startseq\"\n",
    "    for word in word_tokenize(caption):\n",
    "        if (word.lower() not in s):\n",
    "            cleaned.append(word.lower())\n",
    "    for i in cleaned:\n",
    "        if i==cleaned[len(cleaned)-1]:\n",
    "            x=x+\" \"+i+\" endseq\"\n",
    "        else:\n",
    "            x=x+\" \"+ i\n",
    "    return x\n",
    "\n",
    "def create_data(content):\n",
    "    captions = {}\n",
    "    for line in content.split('\\n'):\n",
    "        if line == 'image,caption' or line == '':\n",
    "            continue\n",
    "        cap = line.split(',')[1:]\n",
    "        im = str(line.split(',')[0])\n",
    "        img_id = str(im.split('.')[0])\n",
    "        if img_id not in captions:\n",
    "            captions[img_id] = []\n",
    "        caps = clean(' '.join(cap))\n",
    "        captions[img_id].append(caps)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGG-oOyk-Ywa",
    "outputId": "fc8ad839-ee76-4e4f-c63b-6e0ff80c1f8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = create_data(content)\n",
    "len(cap.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EM3tgXnA00sT",
    "outputId": "61c339e5-464f-4cb2-8869-0d10330582fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq a child in a pink dress is climbing up a set of stairs in an entry way endseq',\n",
       " 'startseq a girl going into a wooden building endseq',\n",
       " 'startseq a little girl climbing into a wooden playhouse endseq',\n",
       " 'startseq a little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq a little girl in a pink dress going into a wooden cabin endseq']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap[list(cap.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "td-m_aLI-HAW"
   },
   "outputs": [],
   "source": [
    "def to_vocabulary(descriptions):\n",
    " all_desc = set()\n",
    " for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    " return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SlgeiP-DJaD",
    "outputId": "dbd633de-91f0-4b58-ca30-907cc7b6e46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8909"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=list(to_vocabulary(cap))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv8IRSFgCZRk"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file_path = '/content/drive/MyDrive/archive/vocab.pkl'\n",
    "with open(pkl_file_path, 'wb') as pkl_file:\n",
    "    pickle.dump(vocab, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6yBc-XA0CZo0"
   },
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    " all_desc = []\n",
    " for key in descriptions.keys():\n",
    "    [all_desc.append(d) for d in descriptions[key]]\n",
    " return all_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tokenizer and creating train and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3BZjMsQEHFfC"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wwoU8jVv_KX3"
   },
   "outputs": [],
   "source": [
    "tokenizer=create_tokenizer(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "huvYsDeGHvvJ"
   },
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "  lines = to_lines(descriptions)\n",
    "  return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2JPya0VG_ACk"
   },
   "outputs": [],
   "source": [
    "def train_test_split(desc, feats):\n",
    "    train_desc = {k: v for i, (k, v) in enumerate(desc.items()) if i < 6000}\n",
    "    test_desc = {k: v for i, (k, v) in enumerate(desc.items()) if i >= 6000}\n",
    "    selected_features = {key: all_features[key] for key in train_desc.keys() & all_features.keys()}\n",
    "    train_feats = {key: selected_features[key] for key in train_desc.keys()}\n",
    "\n",
    "    test_features = {key: all_features[key] for key in test_desc.keys() & all_features.keys()}\n",
    "    test_feats= {key: test_features[key] for key in test_desc.keys()}\n",
    "    return train_desc, test_desc, train_feats, test_feats\n",
    "\n",
    "train_desc, test_desc, train_feats, test_feats = train_test_split(cap, all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjVb93TL-4Re",
    "outputId": "848a113e-2525-42f0-fc30-f74aa7210cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(cap)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "koQwuWGqHxp1"
   },
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "      for i in range(1, len(seq)):\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        X1.append(photo)\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1-7lCf6G1IGu"
   },
   "outputs": [],
   "source": [
    "def define_model(vocab_size,max_caption_length):\n",
    "  img_input=Input(shape=(4096,))\n",
    "  drop=Dropout(0.3)(img_input)\n",
    "  img_encoder=Dense(256, activation='relu')(drop)\n",
    "\n",
    "  text_input = Input(shape=(max_caption_length,))\n",
    "  text_embedding = Embedding(vocab_size, 256, mask_zero=True)(text_input)\n",
    "  drop=Dropout(0.3)(text_embedding)\n",
    "  text_encoder = LSTM(256)(drop)\n",
    "\n",
    "  decoder1 = Add()([img_encoder, text_encoder])\n",
    "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\n",
    "  output = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "  model = Model(inputs=[img_input, text_input], outputs=output)\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    " \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zypCs5_S7l4s"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    " while 1:\n",
    "  for key, desc_list in descriptions.items():\n",
    "    photo = photos[key][0]\n",
    "    in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "    yield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WN5O-RnPSjLI"
   },
   "outputs": [],
   "source": [
    "vocab_len=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOWDYug_tIbG"
   },
   "outputs": [],
   "source": [
    "train_data = create_sequences(tokenizer, max_length, train_desc, train_feats, vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARo0FGYKxmrD",
    "outputId": "aa57d02f-5054-4414-da80-3aefdc5b7417"
   },
   "source": [
    "# **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xse_Hn4-N-r",
    "outputId": "53c7ebbb-f635-4a6b-ddcb-5074d01d74d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 40)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 4096)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 40, 256)              2280704   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 4096)                 0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 40, 256)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  1048832   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 256)                  525312    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n",
      "                                                                     'lstm[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 8909)                 2289613   ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6210253 (23.69 MB)\n",
      "Trainable params: 6210253 (23.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "6000/6000 [==============================] - 1069s 178ms/step - loss: 4.1874 - accuracy: 0.2902\n",
      "6000/6000 [==============================] - 1268s 211ms/step - loss: 3.4676 - accuracy: 0.3392\n",
      "6000/6000 [==============================] - 1428s 238ms/step - loss: 3.2273 - accuracy: 0.3554\n",
      "6000/6000 [==============================] - 1201s 200ms/step - loss: 3.0832 - accuracy: 0.3661\n",
      "6000/6000 [==============================] - 1004s 167ms/step - loss: 2.9707 - accuracy: 0.3745\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_len, max_length)\n",
    "\n",
    "epochs = 5\n",
    "steps = len(train_desc)\n",
    "for i in range(epochs):\n",
    "\n",
    " generator = data_generator(train_desc, train_feats, tokenizer, max_length, vocab_len)\n",
    "\n",
    " model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    " model.save('model_' + str(i) + '.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing and Evaluating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = [],[]\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "\n",
    "    print('BLEU-1:', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2:',corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3:',corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4:',corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    return actual,predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.4851472633042599\n",
      "BLEU-2: 0.3028213363977868\n",
      "BLEU-3: 0.20836074254446565\n",
      "BLEU-4: 0.10134149820114542\n"
     ]
    }
   ],
   "source": [
    "actual,predicted=evaluate_model(model, test_desc, test_feats, tokenizer, max_length)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
